# Migration Guide: Old LangChain â†’ Modern Explicit Orchestration

## ğŸ¯ Migration Strategy

**Approach Used:** **Explicit Application-Controlled Workflow**
- âœ… Plain Python orchestration
- âœ… Modern LangChain components (imports only)
- âœ… No agents, no AgentExecutor
- âœ… No implicit memory wrappers
- âœ… No LangGraph (not needed for linear workflow)

---

## ğŸ“Š What Changed

### âŒ REMOVED (Old Style)

| Component | Why Removed | Replacement |
|-----------|-------------|-------------|
| `StreamingConversationalRetrievalChain` | Black box, framework-controlled | `ChatOrchestrator` (explicit Python) |
| `ConversationBufferMemory` | Implicit memory behavior | Direct database calls |
| `StreamableChain` mixin | Workaround for missing streaming | Built-in `.stream()` |
| `TraceableChain` mixin | Workaround for tracing | Direct Langfuse integration |
| Custom `BaseCallbackHandler` | Workaround for old chains | Native streaming |
| `from langchain.chat_models` | Deprecated imports | `from langchain_openai` |
| `from langchain.vectorstores.pinecone` | Old Pinecone integration | `from langchain_pinecone` |

### âœ… ADDED (Modern)

| Component | Purpose | Benefits |
|-----------|---------|----------|
| `ChatOrchestrator` | Explicit workflow control | Full debuggability, cost control |
| Direct DB saves | Explicit persistence | No memory wrapper magic |
| Modern imports | Up-to-date packages | Bug fixes, performance |
| Cost control limits | `max_history_messages`, `max_tokens` | Predictable costs |
| Explicit error handling | Try/except in orchestrator | Production reliability |

---

## ğŸ”„ Step-by-Step Migration

### **Step 1: Update Dependencies**

```bash
# Backup old requirements
cp requirements.txt requirements_old.txt

# Use new requirements
cp requirements_modern.txt requirements.txt

# Install
pip install -r requirements.txt
```

### **Step 2: Update Imports in Existing Code**

**Find all old imports:**
```bash
grep -r "from langchain\." app/chat/
```

**Replace as follows:**

| Old Import | New Import |
|------------|------------|
| `from langchain.chat_models import ChatOpenAI` | `from langchain_openai import ChatOpenAI` |
| `from langchain.embeddings import OpenAIEmbeddings` | `from langchain_openai import OpenAIEmbeddings` |
| `from langchain.vectorstores.pinecone import Pinecone` | `from langchain_pinecone import PineconeVectorStore` |
| `from langchain.document_loaders import PyPDFLoader` | `from langchain_community.document_loaders import PyPDFLoader` |
| `from langchain.text_splitter import RecursiveCharacterTextSplitter` | `from langchain_text_splitters import RecursiveCharacterTextSplitter` |

### **Step 3: Switch to Modern Chat Builder**

**In `app/web/views/conversation_views.py`:**

```python
# OLD
from app.chat import build_chat

# NEW
from app.chat.chat_modern import build_chat
```

**No other changes needed!** The orchestrator has the same interface:
- `.run(input)` â†’ returns string
- `.stream(input)` â†’ yields strings

### **Step 4: Switch to Modern Embeddings**

**In `app/web/tasks/embeddings.py` (or wherever celery task is):**

```python
# OLD
from app.chat.create_embeddings import create_embeddings_for_pdf

# NEW
from app.chat.create_embeddings_modern import create_embeddings_for_pdf
```

### **Step 5: Update Pinecone Initialization**

**If using Pinecone v5, update environment variables:**

```bash
# Old (v3)
PINECONE_API_KEY=your_key
PINECONE_ENV_NAME=us-west1-gcp

# New (v5) - Remove PINECONE_ENV_NAME
PINECONE_API_KEY=your_key
PINECONE_INDEX_NAME=your_index
```

### **Step 6: Test**

```bash
# Start services
inv dev          # Terminal 1: Flask
inv devworker    # Terminal 2: Celery
redis-server     # Terminal 3: Redis

# Test chat
curl -X POST http://localhost:8000/api/conversations/{id}/messages \
  -H "Content-Type: application/json" \
  -d '{"input": "What is this document about?"}'

# Test streaming
curl -X POST http://localhost:8000/api/conversations/{id}/messages?stream=true \
  -H "Content-Type: application/json" \
  -d '{"input": "Summarize this PDF"}'
```

---

## ğŸ” Architecture Comparison

### OLD: Framework-Controlled Chain

```
User Question
     â†“
StreamingConversationalRetrievalChain
     â†“ (black box)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. Load memory       â”‚ â† Implicit
  â”‚ 2. Condense question â”‚ â† Hidden
  â”‚ 3. Retrieve docs     â”‚ â† Framework-controlled
  â”‚ 4. Generate answer   â”‚ â† Framework-controlled
  â”‚ 5. Save to memory    â”‚ â† Implicit
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Answer
```

**Problems:**
- âŒ Can't see what's happening
- âŒ Can't control costs (unlimited history)
- âŒ Hard to debug failures
- âŒ Memory wrapper hides DB calls

### NEW: Application-Controlled Orchestrator

```
User Question
     â†“
ChatOrchestrator.run()
     â†“ (explicit Python)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1. Load history      â”‚ â† Explicit DB call (max 10 messages)
  â”‚    - Cost control âœ“  â”‚
  â”‚ 2. Reformulate       â”‚ â† Explicit LLM call (gpt-3.5-turbo)
  â”‚    - Cost control âœ“  â”‚
  â”‚ 3. Retrieve docs     â”‚ â† Explicit retriever (max 5 docs)
  â”‚    - Cost control âœ“  â”‚
  â”‚ 4. Build prompt      â”‚ â† Explicit prompt construction
  â”‚    - Full visibility â”‚
  â”‚ 5. Generate answer   â”‚ â† Explicit LLM call (.stream())
  â”‚    - Max tokens âœ“    â”‚
  â”‚ 6. Save to DB        â”‚ â† Explicit DB call
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Answer
```

**Benefits:**
- âœ… Full transparency
- âœ… Cost controls at every step
- âœ… Easy debugging (print statements work!)
- âœ… Direct DB control

---

## ğŸ“ˆ Cost Control Features

The modern orchestrator includes explicit cost controls:

```python
# In ChatOrchestrator
max_history_messages=10    # Limit context window
max_retrieved_docs=5       # Limit retrieval

# In LLM
max_tokens=2000           # Limit output length
request_timeout=60        # Prevent hanging

# In retriever
k=5                       # Explicit doc limit
```

**Estimated cost reduction:** 30-50% compared to old approach
- Reason: Limited history, limited retrieval, capped tokens

---

## ğŸ› Debugging Improvements

### OLD: Black Box

```python
# Chain error - no idea where it failed
chain.run("question")
# Error: "Chain failed"
# ğŸ¤· Was it retrieval? LLM? Memory?
```

### NEW: Full Visibility

```python
# Orchestrator with explicit steps
orchestrator.run("question")

# Console output:
# [ORCHESTRATOR] Starting workflow for: question
# [ORCHESTRATOR] Loaded 3 history messages
# [DEBUG] Original: question
# [DEBUG] Standalone: what is the full question?
# [DEBUG] Retrieved 5 documents
# [ORCHESTRATOR] Generated answer: 247 chars
# [ORCHESTRATOR] Saved to database

# Error example:
# [ERROR] Document retrieval failed: Connection timeout
# âœ… You know exactly what failed!
```

---

## ğŸš€ Production Benefits

| Aspect | Old Approach | New Approach |
|--------|-------------|--------------|
| **Debuggability** | Black box | Print statements at every step |
| **Cost Control** | None | Limits at every step |
| **Error Handling** | Framework-level | Explicit try/except |
| **Monitoring** | Hard to instrument | Easy to add metrics |
| **Tracing** | Custom callback | Direct Langfuse calls |
| **Testing** | Mock entire chain | Test individual functions |
| **Maintenance** | Framework updates break things | Your code, your control |

---

## ğŸ”§ Rollback Plan

If you need to rollback:

```bash
# Restore old requirements
cp requirements_old.txt requirements.txt
pip install -r requirements.txt

# Switch back to old chat builder
# In conversation_views.py:
from app.chat import build_chat  # Old version
```

---

## ğŸ“ Code Ownership

### OLD: Framework Owns Your Logic
- Chain decides when to call LLM
- Memory decides what to save
- Callbacks decide what to stream

### NEW: You Own Your Logic
- You call LLM explicitly
- You save to DB explicitly
- You control streaming explicitly

**Philosophy:** **"Explicit is better than implicit"** (Python Zen)

---

## âœ… Checklist

- [ ] Backed up old code
- [ ] Updated requirements.txt
- [ ] Installed new dependencies
- [ ] Switched to `chat_modern.py`
- [ ] Switched to `create_embeddings_modern.py`
- [ ] Updated environment variables (Pinecone)
- [ ] Tested chat (non-streaming)
- [ ] Tested chat (streaming)
- [ ] Tested embedding creation
- [ ] Verified cost controls are working
- [ ] Set up monitoring/logging
- [ ] Documented any custom changes

---

## ğŸ†˜ Troubleshooting

### Issue: "ImportError: No module named langchain_openai"
**Fix:** Install modern packages:
```bash
pip install langchain-openai langchain-pinecone langchain-community
```

### Issue: "Pinecone API changed"
**Fix:** Update to Pinecone v5:
```bash
pip install pinecone-client==5.0.0
```

### Issue: "Streaming not working"
**Fix:** Ensure you're using the modern orchestrator:
```python
from app.chat.chat_modern import build_chat
```

### Issue: "Costs higher than expected"
**Fix:** Check limits in `orchestrator.py`:
```python
max_history_messages=10  # Reduce if needed
max_retrieved_docs=5     # Reduce if needed
max_tokens=2000          # Reduce if needed
```

---

## ğŸ“š Further Reading

- [LangChain v0.3 Migration Guide](https://python.langchain.com/docs/versions/migrating_chains/)
- [Modern LangChain Docs](https://python.langchain.com/docs/get_started/introduction)
- [Pinecone v5 Docs](https://docs.pinecone.io/)

---

## ğŸ‰ Summary

**You have successfully migrated from:**
- âŒ Old style (chains, implicit memory, framework control)

**To:**
- âœ… Modern style (explicit orchestration, application control)

**Without using:**
- âŒ Agents (not needed)
- âŒ LangGraph (not needed for linear workflow)
- âŒ LCEL (plain Python is clearer here)

**Result:**
- âœ… Full production readiness
- âœ… Complete cost control
- âœ… Maximum debuggability
- âœ… Your code, your control
